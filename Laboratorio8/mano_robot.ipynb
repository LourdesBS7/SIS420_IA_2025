{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22b72b9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptim\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "import math\n",
    "\n",
    "class RoboticArmEnvironment:\n",
    "    \"\"\"\n",
    "    Entorno de simulación para el brazo robótico que debe voltear un vaso.\n",
    "    Simplificado pero con las características esenciales del problema.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Configuración del entorno\n",
    "        self.workspace_size = 1.0  # 1 metro de espacio de trabajo\n",
    "        self.num_joints = 4  # 4 articulaciones del brazo\n",
    "        self.max_steps = 200  # Máximo número de pasos por episodio\n",
    "        \n",
    "        # Estado del brazo (ángulos de articulaciones en radianes)\n",
    "        self.joint_angles = np.zeros(self.num_joints)\n",
    "        self.joint_limits = np.array([[-np.pi, np.pi]] * self.num_joints)\n",
    "        \n",
    "        # Posición del vaso (x, y, z) y orientación\n",
    "        self.cup_position = np.zeros(3)\n",
    "        self.cup_upright = True  # Si el vaso está derecho\n",
    "        \n",
    "        # Posición del end-effector del brazo\n",
    "        self.end_effector_pos = np.zeros(3)\n",
    "        \n",
    "        # Contador de pasos\n",
    "        self.current_step = 0\n",
    "        \n",
    "        # Configuración de dimensiones\n",
    "        self.state_dim = self.num_joints + 6  # articulaciones + pos_vaso + pos_end_effector\n",
    "        self.action_dim = self.num_joints + 1  # movimiento articulaciones + acción empuje\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reinicia el entorno para un nuevo episodio\"\"\"\n",
    "        # Posición inicial aleatoria del brazo\n",
    "        self.joint_angles = np.random.uniform(-0.5, 0.5, self.num_joints)\n",
    "        \n",
    "        # Posición aleatoria del vaso en el espacio de trabajo\n",
    "        self.cup_position = np.array([\n",
    "            np.random.uniform(-0.4, 0.4),  # x\n",
    "            np.random.uniform(0.2, 0.8),   # y (distancia del brazo)\n",
    "            0.05  # z (altura fija sobre la mesa)\n",
    "        ])\n",
    "        self.cup_upright = True\n",
    "        \n",
    "        # Calcular posición inicial del end-effector\n",
    "        self._update_end_effector_position()\n",
    "        \n",
    "        self.current_step = 0\n",
    "        \n",
    "        return self._get_state()\n",
    "    \n",
    "    def _update_end_effector_position(self):\n",
    "        \"\"\"Calcula la posición del end-effector basada en cinemática directa simplificada\"\"\"\n",
    "        # Longitudes de los eslabones (simplificado)\n",
    "        link_lengths = [0.3, 0.25, 0.2, 0.15]\n",
    "        \n",
    "        x = 0\n",
    "        y = 0\n",
    "        z = 0\n",
    "        \n",
    "        current_angle = 0\n",
    "        for i, (angle, length) in enumerate(zip(self.joint_angles, link_lengths)):\n",
    "            current_angle += angle\n",
    "            x += length * np.cos(current_angle)\n",
    "            y += length * np.sin(current_angle)\n",
    "            if i == 0:  # Primera articulación controla altura\n",
    "                z += length * np.sin(angle)\n",
    "        \n",
    "        self.end_effector_pos = np.array([x, y, max(0, z)])\n",
    "    \n",
    "    def _get_state(self):\n",
    "        \"\"\"Retorna el estado actual del entorno\"\"\"\n",
    "        return np.concatenate([\n",
    "            self.joint_angles,\n",
    "            self.cup_position,\n",
    "            self.end_effector_pos\n",
    "        ])\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Ejecuta una acción en el entorno\"\"\"\n",
    "        self.current_step += 1\n",
    "        \n",
    "        # Separar acciones de movimiento y empuje\n",
    "        joint_actions = action[:-1]\n",
    "        push_action = action[-1]\n",
    "        \n",
    "        # Aplicar acciones de movimiento (limitadas)\n",
    "        delta_angles = np.clip(joint_actions * 0.1, -0.1, 0.1)\n",
    "        self.joint_angles += delta_angles\n",
    "        \n",
    "        # Aplicar límites de articulaciones\n",
    "        for i in range(self.num_joints):\n",
    "            self.joint_angles[i] = np.clip(\n",
    "                self.joint_angles[i], \n",
    "                self.joint_limits[i][0], \n",
    "                self.joint_limits[i][1]\n",
    "            )\n",
    "        \n",
    "        # Actualizar posición del end-effector\n",
    "        self._update_end_effector_position()\n",
    "        \n",
    "        # Calcular recompensa\n",
    "        reward = self._calculate_reward(push_action)\n",
    "        \n",
    "        # Verificar si el episodio terminó\n",
    "        done = self._check_done()\n",
    "        \n",
    "        return self._get_state(), reward, done, {}\n",
    "    \n",
    "    def _calculate_reward(self, push_action):\n",
    "        \"\"\"Calcula la recompensa basada en el estado actual\"\"\"\n",
    "        reward = 0\n",
    "        \n",
    "        # Distancia al vaso\n",
    "        distance_to_cup = np.linalg.norm(self.end_effector_pos - self.cup_position)\n",
    "        \n",
    "        # Recompensa por acercarse al vaso\n",
    "        reward += -distance_to_cup * 2.0\n",
    "        \n",
    "        # Recompensa por estar cerca del vaso\n",
    "        if distance_to_cup < 0.05:\n",
    "            reward += 10.0\n",
    "            \n",
    "            # Si está cerca y ejecuta empuje\n",
    "            if push_action > 0.5 and self.cup_upright:\n",
    "                reward += 50.0  # Gran recompensa por voltear el vaso\n",
    "                self.cup_upright = False\n",
    "        \n",
    "        # Penalización por salirse del espacio de trabajo\n",
    "        if (abs(self.end_effector_pos[0]) > 0.5 or \n",
    "            self.end_effector_pos[1] < 0 or \n",
    "            self.end_effector_pos[1] > 1.0 or\n",
    "            self.end_effector_pos[2] < 0):\n",
    "            reward -= 20.0\n",
    "        \n",
    "        # Penalización por cada paso (incentiva eficiencia)\n",
    "        reward -= 0.1\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def _check_done(self):\n",
    "        \"\"\"Verifica si el episodio ha terminado\"\"\"\n",
    "        # Termina si el vaso fue volteado o se alcanzó el límite de pasos\n",
    "        return not self.cup_upright or self.current_step >= self.max_steps\n",
    "\n",
    "\n",
    "class PPOAgent:\n",
    "    \"\"\"\n",
    "    Agente PPO (Proximal Policy Optimization) para controlar el brazo robótico\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, lr=3e-4):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Redes neuronales\n",
    "        self.policy_net = PolicyNetwork(state_dim, action_dim)\n",
    "        self.value_net = ValueNetwork(state_dim)\n",
    "        \n",
    "        # Optimizadores\n",
    "        self.policy_optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=lr)\n",
    "        \n",
    "        # Hiperparámetros PPO\n",
    "        self.gamma = 0.99\n",
    "        self.lambda_gae = 0.95\n",
    "        self.epsilon = 0.2\n",
    "        self.entropy_coef = 0.01\n",
    "        self.value_coef = 0.5\n",
    "        \n",
    "        # Buffer para almacenar experiencias\n",
    "        self.buffer = []\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"Selecciona una acción basada en la política actual\"\"\"\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            mean, std = self.policy_net(state_tensor)\n",
    "            dist = Normal(mean, std)\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action).sum(-1)\n",
    "            \n",
    "        return action.squeeze().numpy(), log_prob.item()\n",
    "    \n",
    "    def store_transition(self, state, action, reward, next_state, done, log_prob):\n",
    "        \"\"\"Almacena una transición en el buffer\"\"\"\n",
    "        self.buffer.append({\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'reward': reward,\n",
    "            'next_state': next_state,\n",
    "            'done': done,\n",
    "            'log_prob': log_prob\n",
    "        })\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"Actualiza la política usando PPO\"\"\"\n",
    "        if len(self.buffer) == 0:\n",
    "            return\n",
    "        \n",
    "        # Procesar experiencias\n",
    "        states = torch.FloatTensor([t['state'] for t in self.buffer])\n",
    "        actions = torch.FloatTensor([t['action'] for t in self.buffer])\n",
    "        rewards = [t['reward'] for t in self.buffer]\n",
    "        dones = [t['done'] for t in self.buffer]\n",
    "        old_log_probs = torch.FloatTensor([t['log_prob'] for t in self.buffer])\n",
    "        \n",
    "        # Calcular returns y advantages usando GAE\n",
    "        returns, advantages = self._compute_gae(rewards, dones, states)\n",
    "        \n",
    "        # Múltiples épocas de entrenamiento\n",
    "        for _ in range(4):\n",
    "            # Calcular probabilidades actuales\n",
    "            mean, std = self.policy_net(states)\n",
    "            dist = Normal(mean, std)\n",
    "            new_log_probs = dist.log_prob(actions).sum(-1)\n",
    "            entropy = dist.entropy().sum(-1)\n",
    "            \n",
    "            # Ratio de probabilidades\n",
    "            ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "            \n",
    "            # Pérdida de política PPO\n",
    "            surr1 = ratio * advantages\n",
    "            surr2 = torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon) * advantages\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "            \n",
    "            # Pérdida de entropía\n",
    "            entropy_loss = -self.entropy_coef * entropy.mean()\n",
    "            \n",
    "            # Pérdida total de política\n",
    "            total_policy_loss = policy_loss + entropy_loss\n",
    "            \n",
    "            # Actualizar política\n",
    "            self.policy_optimizer.zero_grad()\n",
    "            total_policy_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 0.5)\n",
    "            self.policy_optimizer.step()\n",
    "            \n",
    "            # Pérdida de valor\n",
    "            values = self.value_net(states).squeeze()\n",
    "            value_loss = F.mse_loss(values, returns)\n",
    "            \n",
    "            # Actualizar red de valor\n",
    "            self.value_optimizer.zero_grad()\n",
    "            value_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.value_net.parameters(), 0.5)\n",
    "            self.value_optimizer.step()\n",
    "        \n",
    "        # Limpiar buffer\n",
    "        self.buffer.clear()\n",
    "    \n",
    "    def _compute_gae(self, rewards, dones, states):\n",
    "        \"\"\"Calcula Generalized Advantage Estimation\"\"\"\n",
    "        values = self.value_net(states).squeeze().detach().numpy()\n",
    "        \n",
    "        returns = []\n",
    "        advantages = []\n",
    "        gae = 0\n",
    "        \n",
    "        for i in reversed(range(len(rewards))):\n",
    "            if i == len(rewards) - 1:\n",
    "                next_value = 0 if dones[i] else values[i]\n",
    "            else:\n",
    "                next_value = values[i + 1]\n",
    "            \n",
    "            delta = rewards[i] + self.gamma * next_value - values[i]\n",
    "            gae = delta + self.gamma * self.lambda_gae * (1 - dones[i]) * gae\n",
    "            advantages.insert(0, gae)\n",
    "            returns.insert(0, gae + values[i])\n",
    "        \n",
    "        advantages = torch.FloatTensor(advantages)\n",
    "        returns = torch.FloatTensor(returns)\n",
    "        \n",
    "        # Normalizar advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        return returns, advantages\n",
    "\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"Red neuronal para la política\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        \n",
    "        self.mean_head = nn.Linear(64, action_dim)\n",
    "        self.log_std_head = nn.Linear(64, action_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        \n",
    "        mean = torch.tanh(self.mean_head(x))  # Acciones normalizadas\n",
    "        log_std = torch.clamp(self.log_std_head(x), -20, 2)\n",
    "        std = torch.exp(log_std)\n",
    "        \n",
    "        return mean, std\n",
    "\n",
    "\n",
    "class ValueNetwork(nn.Module):\n",
    "    \"\"\"Red neuronal para la función de valor\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        value = self.fc4(x)\n",
    "        \n",
    "        return value\n",
    "\n",
    "\n",
    "def train_agent(episodes=1000):\n",
    "    \"\"\"Función principal de entrenamiento\"\"\"\n",
    "    env = RoboticArmEnvironment()\n",
    "    agent = PPOAgent(env.state_dim, env.action_dim)\n",
    "    \n",
    "    # Métricas de entrenamiento\n",
    "    episode_rewards = []\n",
    "    success_rate = deque(maxlen=100)\n",
    "    \n",
    "    print(\"Iniciando entrenamiento del brazo robótico...\")\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        \n",
    "        for step in range(env.max_steps):\n",
    "            # Seleccionar acción\n",
    "            action, log_prob = agent.get_action(state)\n",
    "            \n",
    "            # Ejecutar acción\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # Almacenar transición\n",
    "            agent.store_transition(state, action, reward, next_state, done, log_prob)\n",
    "            \n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                success_rate.append(1 if not env.cup_upright else 0)\n",
    "                break\n",
    "        \n",
    "        # Actualizar agente cada 32 episodios\n",
    "        if episode % 32 == 0 and episode > 0:\n",
    "            agent.update()\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        \n",
    "        # Mostrar progreso\n",
    "        if episode % 100 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-100:])\n",
    "            avg_success = np.mean(success_rate) if success_rate else 0\n",
    "            print(f\"Episodio {episode}: Recompensa promedio: {avg_reward:.2f}, \"\n",
    "                  f\"Tasa de éxito: {avg_success:.2%}\")\n",
    "    \n",
    "    return agent, episode_rewards, success_rate\n",
    "\n",
    "\n",
    "def evaluate_agent(agent, env, num_episodes=10):\n",
    "    \"\"\"Evalúa el rendimiento del agente entrenado\"\"\"\n",
    "    successes = 0\n",
    "    total_rewards = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        \n",
    "        for step in range(env.max_steps):\n",
    "            action, _ = agent.get_action(state)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                if not env.cup_upright:\n",
    "                    successes += 1\n",
    "                break\n",
    "        \n",
    "        total_rewards.append(episode_reward)\n",
    "    \n",
    "    success_rate = successes / num_episodes\n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    \n",
    "    print(f\"\\n=== EVALUACIÓN FINAL ===\")\n",
    "    print(f\"Tasa de éxito: {success_rate:.2%}\")\n",
    "    print(f\"Recompensa promedio: {avg_reward:.2f}\")\n",
    "    print(f\"Episodios exitosos: {successes}/{num_episodes}\")\n",
    "    \n",
    "    return success_rate, avg_reward\n",
    "\n",
    "\n",
    "def plot_training_progress(episode_rewards, success_rate):\n",
    "    \"\"\"Visualiza el progreso del entrenamiento\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "    \n",
    "    # Recompensas por episodio\n",
    "    ax1.plot(episode_rewards, alpha=0.6, color='blue')\n",
    "    ax1.plot(np.convolve(episode_rewards, np.ones(100)/100, mode='valid'), \n",
    "             color='red', linewidth=2, label='Media móvil (100 episodios)')\n",
    "    ax1.set_title('Recompensa por Episodio')\n",
    "    ax1.set_xlabel('Episodio')\n",
    "    ax1.set_ylabel('Recompensa')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Tasa de éxito\n",
    "    if len(success_rate) > 0:\n",
    "        success_episodes = range(len(episode_rewards) - len(success_rate), len(episode_rewards))\n",
    "        ax2.plot(success_episodes, success_rate, alpha=0.6, color='green')\n",
    "        ax2.set_title('Tasa de Éxito (Últimos 100 episodios)')\n",
    "        ax2.set_xlabel('Episodio')\n",
    "        ax2.set_ylabel('Tasa de Éxito')\n",
    "        ax2.set_ylim(0, 1)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Ejemplo de uso\n",
    "if __name__ == \"__main__\":\n",
    "    # Entrenar el agente\n",
    "    trained_agent, rewards, success_rates = train_agent(episodes=1000)\n",
    "    \n",
    "    # Evaluar el agente entrenado\n",
    "    env = RoboticArmEnvironment()\n",
    "    evaluate_agent(trained_agent, env)\n",
    "    \n",
    "    # Visualizar resultados\n",
    "    plot_training_progress(rewards, list(success_rates))\n",
    "    \n",
    "    print(\"\\n=== SISTEMA COMPLETADO ===\")\n",
    "    print(\"El brazo robótico ha sido entrenado exitosamente!\")\n",
    "    print(\"Características del sistema:\")\n",
    "    print(\"- Algoritmo: PPO (Proximal Policy Optimization)\")\n",
    "    print(\"- Espacio de estados: Posición articulaciones + posición vaso + posición end-effector\")\n",
    "    print(\"- Espacio de acciones: Movimiento articulaciones + acción de empuje\")\n",
    "    print(\"- Objetivo: Localizar y voltear vaso en posición aleatoria\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
